# -*- coding: utf-8 -*-
"""metis partitioning.ipynb

Automatically generated by Colaboratory.


"""

import sys
sys.path.append('../..')
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
import metis
tf.get_logger().setLevel('ERROR')

pip install torchKGE



import torchKGE

from torchKGE.datasets import load_fb15k_237
# load fb15k-237 dataset
dataset = load_fb15k_237()

import numpy as np
import pandas as pd


df = pd.read_csv('/content/sample_data/train.txt', sep='\t', header=None, names=['head', 'relation', 'tail'])


k = 12

s
indices = np.random.permutation(df.index)


partitions = np.array_split(indices, k)


for i, part in enumerate(partitions):
    print(f"Partition {i}: {part}")

import ray

ray.init()

@ray.remote
def my_function(x):
    return x * 2

results = ray.get([my_function.remote(i) for i in range(10)])
print(results)

from torchKGE.latent_features import ScoringBasedEmbeddingModel


tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./transe_train_logs')


model = ScoringBasedEmbeddingModel(eta=5,
                                   k=300,
                                   scoring_type='TransE')

# you can either use optimizers/regularizers/loss/initializers with default values or you can
# import it and customize the hyperparameters and pass it to compile

# Let's create an adam optimizer with customized learning rate =0.005
adam = tf.keras.optimizers.Adam(learning_rate=0.005)
# Let's compile the model with self_advarsarial loss of default parameters
model.compile(optimizer=adam, loss='self_adversarial')

# fit the model to data.
model.fit(dataset['train'],
             batch_size=10000,
             epochs=10,
             callbacks=[tensorboard_callback])

# the training can be visualised using the following command:
# tensorboard --logdir='./transe_train_logs' --port=8891
# open the browser and go to the following URL: http://127.0.0.1:8891/

pred = model.predict(dataset['test'][:5],
                       batch_size=100)
pred

# evaluate on the test set
ranks = model.evaluate(dataset['test'],
                       batch_size=100,
                       corrupt_side='s,o', # corrupt both subject and object
                       use_filter={'train':dataset['train'], # Filter to be used for evaluation
                                   'valid':dataset['valid'],
                                   'test':dataset['test']}
                       )


from torchKGE.evaluation.metrics import mrr_score, hits_at_n_score, mr_score

print('MR:', mr_score(ranks))
print('hits@10:', hits_at_n_score(ranks, 10))
print('hits@5:', hits_at_n_score(ranks, 5))

# evaluate on the test set
ranks = model.evaluate(dataset['test'],     # test set
                       batch_size=100,      # evaluation batch size
                       corrupt_side='s,o'   # sides to corrupt for scoring and ranking
                       )


from torchKGE.evaluation.metrics import  hits_at_n_score, mr_score

# convert the ranks to milliseconds/step
ranks_ms = ranks * 1000

print('MR:', mr_score(ranks))
print('hits@10:', hits_at_n_score(ranks, 10))
print('hits@5:', hits_at_n_score(ranks, 5))

ranks = model.evaluate(dataset['test'],
                       batch_size=100,
                       corrupt_side='o' # corrupt only the object
                       )


from torchKGE.evaluation.metrics import  hits_at_n_score, mr_score

print('MR:', mr_score(ranks))

print('hits@10:', hits_at_n_score(ranks, 10))
print('hits@5:', hits_at_n_score(ranks, 5))

import sys
sys.path.append('../..')
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
tf.get_logger().setLevel('ERROR')
import numpy as np

pip install ampligraph

import torchKGE
# load the dataset
from torchKGE.datasets import load_wn18rr
X = load_wn18rr()



from torchKGE.compat import DistMult

model = transE(batches_count=10, seed=0, epochs=1000, k=350, eta=10,
                    # Use adam optimizer with learning rate 1e-3
                    optimizer='adam', optimizer_params={'lr':1e-3},
                    # Use multiclass_nll loss
                    loss='multiclass_nll', loss_params={},
                    # Use L3 regularizer with regularizer weight 1e-3
                    regularizer='LP', regularizer_params={'p':3, 'lambda':1e-3},
                    # Enable stdout messages (set to false if you don't want to display)
                    verbose=True)


filter = np.concatenate((X['train'], X['valid'][::10], X['test']))


model.fit(X['train'][::2],
          early_stopping = True,
          early_stopping_params = \
                  {
                      'x_valid': X['valid'][::10],  # validation set
                      'criteria':'hits@10',         # Uses hits10 criteria for early stopping
                      'burn_in': 20,                # early stopping kicks in after 100 epochs
                      'check_interval':20,          # validates every 20th epoch
                      'stop_interval':5,            # stops if 5 successive validation checks are bad.
                      'x_filter': filter,           # Use filter for filtering out positives
                      'corruption_entities':'all',  # corrupt using all entities
                      'corrupt_side':'s'            # corrupt only subject
                  }
          )

X_test = X['test']
X_test[:2]

# Score assigned to unseen triples
model.predict(X_test[:2])

# Get embedding of entities
embed = model.get_embeddings(['11647131','02518161'], embedding_type='entity')
print('Embedding size: ', embed.shape[1])
# Notice that the embedding size for ComplEx is double
# compared to the k specified when initializing the model,
# since ComplEx embeddings live in the space of complex numbers.
print('\n Embedding vectors: ')
print(embed)

# get the entity and relation mappings to emb matrix
ent_to_idx, rel_to_idx = model.get_hyperparameter_dict()
len(ent_to_idx), len(rel_to_idx)

# import the evaluate_performance API from compat module
from torchKGE.compat import evaluate_performance
ranks = evaluate_performance(X_test, model, filter_triples=filter, corrupt_side='s,o', verbose=True)

# import the evaluation metrics
from torchKGE.evaluation.metrics import mrr_score, hits_at_n_score, mr_score

print('MR:', mr_score(ranks))
print('hits@10:', hits_at_n_score(ranks, 10))
print('hits@5:', hits_at_n_score(ranks, 5))

import sys
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
tf.get_logger().setLevel('ERROR')
import numpy as np
import ampligraph

# Import the KGE model
from torchKGE.latent_features import ScoringBasedEmbeddingModel

PATH_TO_DATASET = 'your/path/to/dataset/'

# create the model with transe scoring function
partitioned_model = ScoringBasedEmbeddingModel(eta=2,
                                               k=50,
                                               scoring_type='TransE')
partitioned_model.compile(optimizer='adam', loss='multiclass_nll')

# Here we have specified the path of the input file
# you can also load using default dataloaders load_fb15k_237() and pass numpy array inputs
partitioned_model.fit(PATH_TO_DATASET + 'wn18RR/train.txt',
                      batch_size=10000,
                      partitioning_k=3, # set flag to partition the inputs
                      epochs=10)

import sys
sys.path.append('../..')
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'
os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'
import tensorflow as tf
tf.get_logger().setLevel('ERROR')

import torchKGE
# Benchmark datasets are under ampligraph.datasets module
from torchKGE.datasets import load_fb15k_237
# load fb15k-237 dataset
dataset = load_fb15k_237()

# Import the KGE model
from torchKGE.latent_features import ScoringBasedEmbeddingModel

# you can continue training from where you left after restoring the model
tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir='./transe_train_logs')

# create the model with transe scoring function
model = ScoringBasedEmbeddingModel(eta=5,
                                   k=300,
                                   scoring_type='TransE')

# you can either use optimizers/regularizers/loss/initializers with default values or you can
# import it and customize the hyperparameters and pass it to compile

# Let's create an adam optimizer with customized learning rate =0.005
adam = tf.keras.optimizers.Adam(learning_rate=0.004295)
# Let's compile the model with self_advarsarial loss of default parameters
model.compile(optimizer=adam, loss='self_adversarial')

# fit the model to data.
model.fit(dataset['train'],
             batch_size=1164,
             epochs=10,
             callbacks=[tensorboard_callback])

# the training can be visualised using the following command:
# tensorboard --logdir='./transe_train_logs' --port=8891
# open the browser and go to the following URL: http://127.0.0.1:8891/

# evaluate on the test set
ranks = model.evaluate(dataset['test'],
                       batch_size=100,
                       corrupt_side='o' # corrupt only the object
                       )

# import the evaluation metrics
from torchKGE.evaluation.metrics import  hits_at_n_score, mr_score, mrr_score

print('MR:', mr_score(ranks))
print('hits@10:', hits_at_n_score(ranks, 10))
print('hits@5:', hits_at_n_score(ranks, 5))
